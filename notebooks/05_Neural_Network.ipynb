{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 7. Neuronales Netz für Regression\n",
    "\n",
    "**KI1-Projekt 308** — California Housing Datensatz\n",
    "\n",
    "**Schwerpunkt P5 (Kernaufgabe):** Neuronales Netz mit TensorFlow/Keras\n",
    "für die Vorhersage von Hauspreisen. Vergleich mit linearer Regression.\n",
    "\n",
    "Vorlage: Blatt 12 (TensorFlow Regression), Kapitel 8 Folien\n",
    "\n",
    "> ⚠️ **Python-Version:** Dieses Notebook benötigt **Python 3.13**.  \n",
    "> TensorFlow (≥ 2.18) ist **nicht** kompatibel mit Python 3.14+.  \n",
    "> Venv erstellen mit: `python3.13 -m venv .venv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from utils.data import load_and_clean_data, get_train_test_split\n",
    "from utils.evaluation import evaluate_predictions, add_result\n",
    "from utils.plotting import plot_predicted_vs_actual, plot_residuals, save_fig\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 7.1 Daten laden (skaliert)\n",
    "\n",
    "Neuronale Netze benötigen skalierte Eingabedaten für effizientes Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_and_clean_data()\n",
    "X_train, X_test, y_train, y_test, scaler, feature_names = get_train_test_split(df, scaler='standard')\n",
    "\n",
    "# Validierungssplit aus Trainingsdaten\n",
    "val_split = int(0.8 * len(X_train))\n",
    "X_val, y_val = X_train[val_split:], y_train[val_split:]\n",
    "X_train_nn, y_train_nn = X_train[:val_split], y_train[:val_split]\n",
    "\n",
    "print(f\"Training:   {X_train_nn.shape}\")\n",
    "print(f\"Validation: {X_val.shape}\")\n",
    "print(f\"Test:       {X_test.shape}\")\n",
    "print(f\"Features:   {feature_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 7.2 Referenz: Lineare Regression\n",
    "\n",
    "Aufgabenstellung fordert explizit den Vergleich mit linearer Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "result_lr = evaluate_predictions(\n",
    "    y_train, lr.predict(X_train),\n",
    "    y_test, lr.predict(X_test),\n",
    "    \"Lineare Regression (Referenz)\"\n",
    ")\n",
    "add_result(result_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 7.3 Modell 1: Einfaches NN (1 Hidden Layer)\n",
    "\n",
    "Zunächst ein minimales Netz als Ausgangspunkt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hidden_layers, activation='relu', learning_rate=0.001):\n",
    "    \"\"\"Erstelle ein Sequential-Modell mit gegebener Architektur.\"\"\"\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(X_train.shape[1],)))\n",
    "    \n",
    "def build_model(hidden_layers, activation='relu', learning_rate=0.001):\n",
    "    \"\"\"Erstelle ein Sequential-Modell mit gegebener Architektur.\"\"\"\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(X_train_nn.shape[1],)))  # besser X_train_nn\n",
    "    \n",
    "    for units in hidden_layers:\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                units,\n",
    "                activation=activation,\n",
    "                kernel_regularizer=keras.regularizers.l2(0.001)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    model.add(layers.Dense(1))  # Regression Output\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='mse',\n",
    "        metrics=['mae'],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(model, name, epochs=300, batch_size=32, verbose=0):\n",
    "\n",
    "    # Early Stopping\n",
    "    early_stop = keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=20,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    history = model.fit(\n",
    "        X_train_nn, y_train_nn,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=verbose,\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "\n",
    "    # Vorhersagen\n",
    "    y_train_pred = model.predict(X_train_nn, verbose=0).flatten()\n",
    "    y_test_pred = model.predict(X_test, verbose=0).flatten()\n",
    "\n",
    "   \n",
    "    result = evaluate_predictions(\n",
    "        y_train_nn,     \n",
    "        y_train_pred,\n",
    "        y_test,\n",
    "        y_test_pred,\n",
    "        name\n",
    "    )\n",
    "\n",
    "    add_result(result)\n",
    "\n",
    "    return history, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = build_model([32], activation='relu', learning_rate=0.001)\n",
    "model_1.summary()\n",
    "\n",
    "history_1, result_1 = train_and_evaluate(model_1, \"NN [32] ReLU\", epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tatsächliche Epochen:\", len(history_1.history[\"loss\"])) #sofern die epochen nicht dem maximalen Wert entsprechen wurde overfitting verhidnert "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### 7.3.1 Vergleich mit und ohne L2-Regularisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_plain(hidden_layers, activation='relu', learning_rate=0.001):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(X_train_nn.shape[1],)))\n",
    "    \n",
    "    for units in hidden_layers:\n",
    "        model.add(layers.Dense(units, activation=activation))\n",
    "    \n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_l2(hidden_layers, activation='relu', learning_rate=0.001, l2_value=0.001):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(X_train_nn.shape[1],)))\n",
    "    \n",
    "    for units in hidden_layers:\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                units,\n",
    "                activation=activation,\n",
    "                kernel_regularizer=keras.regularizers.l2(l2_value)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Modelle Trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plain Modell\n",
    "model_plain = build_model_plain([64, 32])\n",
    "history_plain, result_plain = train_and_evaluate(\n",
    "    model_plain,\n",
    "    \"NN Plain\",\n",
    "    epochs=300\n",
    ")\n",
    "\n",
    "# L2 Modell\n",
    "model_l2 = build_model_l2([64, 32])\n",
    "history_l2, result_l2 = train_and_evaluate(\n",
    "    model_l2,\n",
    "    \"NN L2\",\n",
    "    epochs=300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Parameterzahl vergleichen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plain Parameter:\", model_plain.count_params())\n",
    "print(\"L2 Parameter:\", model_l2.count_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Tatsächliche Epochen vergleichen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plain Epochen:\", len(history_plain.history[\"loss\"]))\n",
    "print(\"L2 Epochen:\", len(history_l2.history[\"loss\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Overfitting differnez berechnen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overfit_gap(result):\n",
    "    return result[\"R² Train\"] - result[\"R² Test\"]\n",
    "\n",
    "print(\"Plain Overfit Gap:\", overfit_gap(result_plain))\n",
    "print(\"L2 Overfit Gap:\", overfit_gap(result_l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Lernkurven vergleichen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.plot(history_plain.history[\"val_loss\"], label=\"Plain Val Loss\")\n",
    "plt.plot(history_l2.history[\"val_loss\"], label=\"L2 Val Loss\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "plt.title(\"Validation Loss Comparison\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Residuenvergleich "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_plain = model_plain.predict(X_test).flatten()\n",
    "y_test_pred_l2 = model_l2.predict(X_test).flatten()\n",
    "\n",
    "res_plain = y_test - y_test_pred_plain\n",
    "res_l2 = y_test - y_test_pred_l2\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(y_test_pred_plain, res_plain, alpha=0.4, label=\"Plain\")\n",
    "plt.scatter(y_test_pred_l2, res_l2, alpha=0.4, label=\"L2\")\n",
    "plt.axhline(0)\n",
    "plt.legend()\n",
    "plt.title(\"Residual Comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "#### Vergleich der Modelle mit und ohne L2-Regularisierung:\n",
    "\n",
    "Das neuronale Netz ohne Regularisierung (Plain) erreicht einen Test-R² von 0.7637 bei einem Overfitting-Gap von ca. 0.040. Das Modell mit L2-Regularisierung erzielt hingegen einen leicht höheren Test-R² von 0.7715 und weist mit ca. 0.024 einen deutlich geringeren Overfitting-Gap auf.\n",
    "\n",
    "Die L2-Regularisierung reduziert somit die Differenz zwischen Trainings- und Testleistung und verbessert gleichzeitig die Generalisierungsfähigkeit des Modells. Auch die Validierungskurve verläuft stabiler und zeigt weniger Schwankungen als beim unregularisierten Modell.\n",
    "\n",
    "Die Residuenverteilung beider Modelle ist insgesamt ähnlich, jedoch wirkt das L2-Modell homogener und zeigt eine etwas gleichmäßigere Streuung.\n",
    "\n",
    "Insgesamt deutet dies darauf hin, dass die L2-Regularisierung Overfitting erfolgreich reduziert und die Modellstabilität erhöht, ohne die Vorhersageleistung zu verschlechtern. Das L2-Modell stellt daher in diesem Vergleich die robustere Variante dar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### 7.3.2 Vergleich von Plain L2 Dropout und L2 + Dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Modellfunktionen\n",
    "\n",
    "\n",
    "def build_model_plain(hidden_layers, activation='relu', learning_rate=0.001):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(X_train_nn.shape[1],)))\n",
    "    \n",
    "    for units in hidden_layers:\n",
    "        model.add(layers.Dense(units, activation=activation))\n",
    "    \n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_l2(hidden_layers, activation='relu', learning_rate=0.001, l2_value=0.001):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(X_train_nn.shape[1],)))\n",
    "    \n",
    "    for units in hidden_layers:\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                units,\n",
    "                activation=activation,\n",
    "                kernel_regularizer=keras.regularizers.l2(l2_value)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_dropout(hidden_layers, activation='relu', learning_rate=0.001, dropout_rate=0.2):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(X_train_nn.shape[1],)))\n",
    "    \n",
    "    for units in hidden_layers:\n",
    "        model.add(layers.Dense(units, activation=activation))\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_l2_dropout(hidden_layers, activation='relu', learning_rate=0.001, l2_value=0.001, dropout_rate=0.2):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(X_train_nn.shape[1],)))\n",
    "    \n",
    "    for units in hidden_layers:\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                units,\n",
    "                activation=activation,\n",
    "                kernel_regularizer=keras.regularizers.l2(l2_value)\n",
    "            )\n",
    "        )\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# 2) Overfit-Funktion (mit R²)\n",
    "\n",
    "\n",
    "def overfit_gap(result):\n",
    "    return result[\"R² Train\"] - result[\"R² Test\"]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) Modelle trainieren\n",
    "\n",
    "\n",
    "model_plain = build_model_plain([64, 32])\n",
    "history_plain, result_plain = train_and_evaluate(model_plain, \"NN Plain\", epochs=300)\n",
    "\n",
    "model_l2 = build_model_l2([64, 32])\n",
    "history_l2, result_l2 = train_and_evaluate(model_l2, \"NN L2\", epochs=300)\n",
    "\n",
    "model_dropout = build_model_dropout([64, 32])\n",
    "history_dropout, result_dropout = train_and_evaluate(model_dropout, \"NN Dropout\", epochs=300)\n",
    "\n",
    "model_l2_dropout = build_model_l2_dropout([64, 32])\n",
    "history_l2_dropout, result_l2_dropout = train_and_evaluate(model_l2_dropout, \"NN L2 + Dropout\", epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4) Vergleichstabelle (mit R²)\n",
    "\n",
    "\n",
    "model_results = [\n",
    "    {\n",
    "        \"Modell\": \"NN Plain\",\n",
    "        \"R² Train\": result_plain[\"R² Train\"],\n",
    "        \"R² Test\": result_plain[\"R² Test\"],\n",
    "        \"Overfit Gap\": overfit_gap(result_plain),\n",
    "        \"MAE Test\": result_plain[\"MAE Test\"],\n",
    "        \"Epochen\": len(history_plain.history[\"loss\"]),\n",
    "        \"Parameter\": model_plain.count_params()\n",
    "    },\n",
    "    {\n",
    "        \"Modell\": \"NN L2\",\n",
    "        \"R² Train\": result_l2[\"R² Train\"],\n",
    "        \"R² Test\": result_l2[\"R² Test\"],\n",
    "        \"Overfit Gap\": overfit_gap(result_l2),\n",
    "        \"MAE Test\": result_l2[\"MAE Test\"],\n",
    "        \"Epochen\": len(history_l2.history[\"loss\"]),\n",
    "        \"Parameter\": model_l2.count_params()\n",
    "    },\n",
    "    {\n",
    "        \"Modell\": \"NN Dropout\",\n",
    "        \"R² Train\": result_dropout[\"R² Train\"],\n",
    "        \"R² Test\": result_dropout[\"R² Test\"],\n",
    "        \"Overfit Gap\": overfit_gap(result_dropout),\n",
    "        \"MAE Test\": result_dropout[\"MAE Test\"],\n",
    "        \"Epochen\": len(history_dropout.history[\"loss\"]),\n",
    "        \"Parameter\": model_dropout.count_params()\n",
    "    },\n",
    "    {\n",
    "        \"Modell\": \"NN L2 + Dropout\",\n",
    "        \"R² Train\": result_l2_dropout[\"R² Train\"],\n",
    "        \"R² Test\": result_l2_dropout[\"R² Test\"],\n",
    "        \"Overfit Gap\": overfit_gap(result_l2_dropout),\n",
    "        \"MAE Test\": result_l2_dropout[\"MAE Test\"],\n",
    "        \"Epochen\": len(history_l2_dropout.history[\"loss\"]),\n",
    "        \"Parameter\": model_l2_dropout.count_params()\n",
    "    }\n",
    "]\n",
    "\n",
    "comparison_df = pd.DataFrame(model_results).round(4)\n",
    "\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "Vergleich der verschiedenen Modellvarianten:\n",
    "\n",
    "Das neuronale Netz ohne Regularisierung (NN Plain) erzielt mit einem Test-R² von 0.7911 die beste Vorhersageleistung. Allerdings weist es mit einem Overfitting-Gap von 0.0600 die größte Differenz zwischen Trainings- und Testleistung auf.\n",
    "\n",
    "Die L2-Regularisierung reduziert das Overfitting deutlich (Gap 0.0243), führt jedoch zu einer leicht geringeren Testperformance (R² = 0.7860). Das Dropout-Modell verringert das Overfitting weiter, erreicht jedoch ebenfalls eine geringere Testleistung. Die Kombination aus L2 und Dropout zeigt das geringste Overfitting, allerdings auch die niedrigste Testperformance.\n",
    "\n",
    "Insgesamt liefert das Plain-Modell die höchste Genauigkeit, während L2-Regularisierung die beste Balance zwischen Performance und Generalisierungsfähigkeit bietet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## 7.4 Modell 2: Tieferes Netz (3 Hidden Layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = build_model([64, 32, 16], activation='relu', learning_rate=0.001)\n",
    "model_2.summary()\n",
    "\n",
    "history_2, result_2 = train_and_evaluate(model_2, \"NN [64,32,16] ReLU\", epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## 7.5 Modell 3: Breiteres Netz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = build_model([128, 64, 32], activation='relu', learning_rate=0.001)\n",
    "model_3.summary()\n",
    "\n",
    "history_3, result_3 = train_and_evaluate(model_3, \"NN [128,64,32] ReLU\", epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## 7.6 Modell 4: Verschiedene Aktivierungsfunktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_activation = {}\n",
    "\n",
    "for act in ['relu', 'elu', 'tanh', 'sigmoid']:\n",
    "    model = build_model([64, 32, 16], activation=act, learning_rate=0.001)\n",
    "    history, result = train_and_evaluate(model, f\"NN [64,32,16] {act}\", epochs=200)\n",
    "    results_activation[act] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## 7.7 Modell 5: Tiefes Netz mit Regularisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5 = keras.Sequential([\n",
    "    layers.Input(shape=(X_train.shape[1],)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1),\n",
    "])\n",
    "\n",
    "model_5.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae'],\n",
    ")\n",
    "\n",
    "model_5.summary()\n",
    "history_5, result_5 = train_and_evaluate(model_5, \"NN [128,64,32,16] + Dropout\", epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "## 7.8 Lernkurven visualisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(histories, names):\n",
    "    \"\"\"Lernkurven mehrerer Modelle.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    for hist, name in zip(histories, names):\n",
    "        axes[0].plot(hist.history['loss'], label=f'{name} (Train)')\n",
    "        axes[0].plot(hist.history['val_loss'], '--', label=f'{name} (Val)')\n",
    "    \n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('MSE Loss')\n",
    "    axes[0].set_title('Lernkurven: Loss')\n",
    "    axes[0].legend(fontsize=8)\n",
    "    axes[0].set_yscale('log')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    for hist, name in zip(histories, names):\n",
    "        axes[1].plot(hist.history['mae'], label=f'{name} (Train)')\n",
    "        axes[1].plot(hist.history['val_mae'], '--', label=f'{name} (Val)')\n",
    "    \n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('MAE')\n",
    "    axes[1].set_title('Lernkurven: MAE')\n",
    "    axes[1].legend(fontsize=8)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    save_fig(fig, 'nn_learning_curves')\n",
    "    return fig\n",
    "\n",
    "fig = plot_training_history(\n",
    "    [history_1, history_2, history_3, history_5],\n",
    "    ['[32]', '[64,32,16]', '[128,64,32]', '[128,64,32,16]+Drop']\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## 7.9 Vergleich NN vs. Lineare Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bestes NN-Modell vs. Lineare Regression\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "plot_predicted_vs_actual(\n",
    "    y_test, lr.predict(X_test),\n",
    "    title=\"Lineare Regression\", ax=axes[0]\n",
    ")\n",
    "\n",
    "# Das beste NN-Modell (manuell auswählen nach Ergebnissen)\n",
    "best_nn = model_5  # Anpassen je nach Ergebnis\n",
    "y_pred_nn = best_nn.predict(X_test, verbose=0).flatten()\n",
    "plot_predicted_vs_actual(\n",
    "    y_test, y_pred_nn,\n",
    "    title=\"Neuronales Netz (bestes Modell)\", ax=axes[1]\n",
    ")\n",
    "\n",
    "fig.suptitle(\"Kernvergleich: NN vs. Lineare Regression\", fontsize=14)\n",
    "fig.tight_layout()\n",
    "save_fig(fig, 'nn_vs_linear_regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## 7.10 Zusammenfassung\n",
    "\n",
    "| Aspekt | Lineare Regression | Neuronales Netz |\n",
    "|--------|-------------------|-----------------|\n",
    "| R² Test | _eintragen_ | _eintragen_ |\n",
    "| MAE Test | _eintragen_ | _eintragen_ |\n",
    "| Trainingszeit | < 1s | _eintragen_ |\n",
    "| Interpretierbarkeit | Hoch (Koeffizienten) | Gering (Black Box) |\n",
    "| Hyperparameter | Keine | Architektur, LR, Epochs, ... |\n",
    "\n",
    "**Fazit:**\n",
    "- _Hier Ergebnisse und Interpretation eintragen_\n",
    "- _Wann lohnt sich ein NN gegenüber linearer Regression?_\n",
    "- _Grenzen und offene Fragen_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
