{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 7. Neuronales Netz für Regression\n",
    "\n",
    "**KI1-Projekt 308** — California Housing Datensatz\n",
    "\n",
    "**Schwerpunkt P5 (Kernaufgabe):** Neuronales Netz mit TensorFlow/Keras\n",
    "für die Vorhersage von Hauspreisen. Vergleich mit linearer Regression.\n",
    "\n",
    "Vorlage: Blatt 12 (TensorFlow Regression), Kapitel 8 Folien\n",
    "\n",
    "> ⚠️ **Python-Version:** Dieses Notebook benötigt **Python 3.13**.  \n",
    "> TensorFlow (≥ 2.18) ist **nicht** kompatibel mit Python 3.14+.  \n",
    "> Venv erstellen mit: `python3.13 -m venv .venv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from utils.data import load_and_clean_data, get_train_test_split\n",
    "from utils.evaluation import evaluate_predictions, add_result\n",
    "from utils.plotting import plot_predicted_vs_actual, plot_residuals, save_fig\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 7.1 Daten laden (skaliert)\n",
    "\n",
    "Neuronale Netze benötigen skalierte Eingabedaten für effizientes Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_and_clean_data()\n",
    "X_train, X_test, y_train, y_test, scaler, feature_names = get_train_test_split(df, scaler='standard')\n",
    "\n",
    "# Validierungssplit aus Trainingsdaten\n",
    "val_split = int(0.8 * len(X_train))\n",
    "X_val, y_val = X_train[val_split:], y_train[val_split:]\n",
    "X_train_nn, y_train_nn = X_train[:val_split], y_train[:val_split]\n",
    "\n",
    "print(f\"Training:   {X_train_nn.shape}\")\n",
    "print(f\"Validation: {X_val.shape}\")\n",
    "print(f\"Test:       {X_test.shape}\")\n",
    "print(f\"Features:   {feature_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 7.2 Referenz: Lineare Regression\n",
    "\n",
    "Aufgabenstellung fordert explizit den Vergleich mit linearer Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "result_lr = evaluate_predictions(\n",
    "    y_train, lr.predict(X_train),\n",
    "    y_test, lr.predict(X_test),\n",
    "    \"Lineare Regression (Referenz)\"\n",
    ")\n",
    "add_result(result_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 7.3 Modell 1: Einfaches NN (1 Hidden Layer)\n",
    "\n",
    "Zunächst ein minimales Netz als Ausgangspunkt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hidden_layers, activation='relu', learning_rate=0.001):\n",
    "    \"\"\"Erstelle ein Sequential-Modell mit gegebener Architektur.\"\"\"\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(X_train.shape[1],)))\n",
    "    \n",
    "    for units in hidden_layers:\n",
    "        model.add(layers.Dense(units, activation=activation))\n",
    "    \n",
    "    model.add(layers.Dense(1))  # Regression Output\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='mse',\n",
    "        metrics=['mae'],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, name, epochs=100, batch_size=32, verbose=0):\n",
    "    \"\"\"Trainiere und evaluiere ein Keras-Modell.\"\"\"\n",
    "    history = model.fit(\n",
    "        X_train_nn, y_train_nn,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    \n",
    "    y_train_pred = model.predict(X_train, verbose=0).flatten()\n",
    "    y_test_pred = model.predict(X_test, verbose=0).flatten()\n",
    "    \n",
    "    result = evaluate_predictions(y_train, y_train_pred, y_test, y_test_pred, name)\n",
    "    add_result(result)\n",
    "    \n",
    "    return history, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = build_model([32], activation='relu', learning_rate=0.001)\n",
    "model_1.summary()\n",
    "\n",
    "history_1, result_1 = train_and_evaluate(model_1, \"NN [32] ReLU\", epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 7.4 Modell 2: Tieferes Netz (3 Hidden Layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = build_model([64, 32, 16], activation='relu', learning_rate=0.001)\n",
    "model_2.summary()\n",
    "\n",
    "history_2, result_2 = train_and_evaluate(model_2, \"NN [64,32,16] ReLU\", epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 7.5 Modell 3: Breiteres Netz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = build_model([128, 64, 32], activation='relu', learning_rate=0.001)\n",
    "model_3.summary()\n",
    "\n",
    "history_3, result_3 = train_and_evaluate(model_3, \"NN [128,64,32] ReLU\", epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 7.6 Modell 4: Verschiedene Aktivierungsfunktionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_activation = {}\n",
    "\n",
    "for act in ['relu', 'elu', 'tanh', 'sigmoid']:\n",
    "    model = build_model([64, 32, 16], activation=act, learning_rate=0.001)\n",
    "    history, result = train_and_evaluate(model, f\"NN [64,32,16] {act}\", epochs=200)\n",
    "    results_activation[act] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 7.7 Modell 5: Tiefes Netz mit Regularisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5 = keras.Sequential([\n",
    "    layers.Input(shape=(X_train.shape[1],)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1),\n",
    "])\n",
    "\n",
    "model_5.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae'],\n",
    ")\n",
    "\n",
    "model_5.summary()\n",
    "history_5, result_5 = train_and_evaluate(model_5, \"NN [128,64,32,16] + Dropout\", epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 7.8 Lernkurven visualisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(histories, names):\n",
    "    \"\"\"Lernkurven mehrerer Modelle.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    for hist, name in zip(histories, names):\n",
    "        axes[0].plot(hist.history['loss'], label=f'{name} (Train)')\n",
    "        axes[0].plot(hist.history['val_loss'], '--', label=f'{name} (Val)')\n",
    "    \n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('MSE Loss')\n",
    "    axes[0].set_title('Lernkurven: Loss')\n",
    "    axes[0].legend(fontsize=8)\n",
    "    axes[0].set_yscale('log')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    for hist, name in zip(histories, names):\n",
    "        axes[1].plot(hist.history['mae'], label=f'{name} (Train)')\n",
    "        axes[1].plot(hist.history['val_mae'], '--', label=f'{name} (Val)')\n",
    "    \n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('MAE')\n",
    "    axes[1].set_title('Lernkurven: MAE')\n",
    "    axes[1].legend(fontsize=8)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    save_fig(fig, 'nn_learning_curves')\n",
    "    return fig\n",
    "\n",
    "fig = plot_training_history(\n",
    "    [history_1, history_2, history_3, history_5],\n",
    "    ['[32]', '[64,32,16]', '[128,64,32]', '[128,64,32,16]+Drop']\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 7.9 Vergleich NN vs. Lineare Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bestes NN-Modell vs. Lineare Regression\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "plot_predicted_vs_actual(\n",
    "    y_test, lr.predict(X_test),\n",
    "    title=\"Lineare Regression\", ax=axes[0]\n",
    ")\n",
    "\n",
    "# Das beste NN-Modell (manuell auswählen nach Ergebnissen)\n",
    "best_nn = model_5  # Anpassen je nach Ergebnis\n",
    "y_pred_nn = best_nn.predict(X_test, verbose=0).flatten()\n",
    "plot_predicted_vs_actual(\n",
    "    y_test, y_pred_nn,\n",
    "    title=\"Neuronales Netz (bestes Modell)\", ax=axes[1]\n",
    ")\n",
    "\n",
    "fig.suptitle(\"Kernvergleich: NN vs. Lineare Regression\", fontsize=14)\n",
    "fig.tight_layout()\n",
    "save_fig(fig, 'nn_vs_linear_regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## 7.10 Zusammenfassung\n",
    "\n",
    "| Aspekt | Lineare Regression | Neuronales Netz (bestes Modell) |\n",
    "|--------|-------------------|-----------------|\n",
    "| R² Test | 0.6326 | 0.7795 |\n",
    "| MAE Test | 0.4341 | 0.3066 |\n",
    "| Trainingszeit | < 1s | ~1–2 min (300 Epochs, CPU) |\n",
    "| Interpretierbarkeit | Hoch (Koeffizienten) | Gering (Black Box) |\n",
    "| Hyperparameter | Keine | Architektur, LR, Epochs, Dropout, ... |\n",
    "\n",
    "> Bestes NN-Modell: **[128, 64, 32, 16] + Dropout** (Modell 5, 300 Epochs)\n",
    "\n",
    "**Fazit:**\n",
    "\n",
    "- Das neuronale Netz ([128,64,32,16] + Dropout) übertrifft die lineare Regression deutlich: R² steigt von **0.63 auf 0.78**, der MAE sinkt von **0.43 auf 0.31** (in 100.000 USD). Damit erklärt das NN rund 18 Prozentpunkte mehr Varianz im Testset.\n",
    "\n",
    "- Ein NN lohnt sich gegenüber linearer Regression, wenn die Beziehung zwischen Features und Zielvariable **nichtlinear** ist – wie beim California-Housing-Datensatz, bei dem Faktoren wie Meeresnähe, Einkommensniveau oder Haushaltsgröße komplex zusammenwirken. Die lineare Regression kann diese Interaktionen ohne explizites Feature-Engineering nicht abbilden.\n",
    "\n",
    "- **Grenzen und offene Fragen:**\n",
    "  - Das NN benötigt wesentlich mehr Rechenzeit und Hyperparameter-Tuning (Architektur, Lernrate, Dropout-Rate, Batch-Größe).\n",
    "  - Trotz Dropout zeigt Modell 3 ([128,64,32] ohne Regularisierung) stärkeres Overfitting (R² Train = 0.877 vs. R² Test = 0.726), während Modell 5 durch Dropout robuster generalisiert.\n",
    "  - Interpretierbarkeit fehlt beim NN vollständig – für regulierte Domänen (z. B. Kreditvergabe, Medizin) bleibt lineare Regression oft bevorzugt.\n",
    "  - Weitere Verbesserungen wären durch Early Stopping, Learning-Rate-Scheduling oder Ensembling mit anderen Modellen (Random Forest, Gradient Boosting) möglich.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
