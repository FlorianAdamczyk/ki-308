{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 3. Regularisierte Regression: LASSO & Ridge\n",
    "\n",
    "**KI1-Projekt 308** — California Housing Datensatz\n",
    "\n",
    "**Schwerpunkt P1:** Feature-Selektion via LASSO, Ridge-Regression,\n",
    "polynomiale Feature-Transformation, Skalierungseffekte.\n",
    "\n",
    "Vorlage: HA9/10 Aufgabe 3 und HA11 Aufgabe 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LassoCV, Ridge, RidgeCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from utils.data import load_and_clean_data, get_train_test_split\n",
    "from utils.evaluation import evaluate_model, add_result\n",
    "from utils.plotting import plot_predicted_vs_actual, plot_residuals, save_fig\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 3.1 Daten laden (ohne Skalierung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_and_clean_data()\n",
    "X_train, X_test, y_train, y_test, feature_names = get_train_test_split(df)\n",
    "print(f\"Features: {feature_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 3.2 LassoCV — Feature-Selektion ohne Skalierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_cv = LassoCV(cv=5, random_state=42, max_iter=10000)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Optimales alpha: {lasso_cv.alpha_:.6f}\\n\")\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Koeffizient': lasso_cv.coef_\n",
    "})\n",
    "print(\"LASSO Koeffizienten:\")\n",
    "print(coef_df.to_string(index=False))\n",
    "\n",
    "selected_features = coef_df[coef_df['Koeffizient'] != 0]['Feature'].tolist()\n",
    "print(f\"\\nSelektierte Features ({len(selected_features)}/{len(feature_names)}): {selected_features}\")\n",
    "\n",
    "result_lasso = evaluate_model(lasso_cv, X_train, X_test, y_train, y_test, \"LassoCV (ohne Skalierung)\")\n",
    "add_result(result_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Auswertung: LassoCV ohne Skalierung\n",
    "\n",
    "Das optimale Regularisierungsparameter beträgt α = 0.010325. Bei diesem Wert setzt LASSO den Koeffizienten von **AveBedrms** exakt auf 0 und selektiert damit **7 von 8 Features**. Der stärkste positive Einfluss geht vom Medianeinkommen (`MedInc`, +0.386) aus, während geographische Lage (`Latitude`, –0.374; `Longitude`, –0.361) und Belegungsdichte (`AveOccup`, –0.344) stark negativ wirken. Dies ist plausibel: Küstennahe, einkommensstarke Gebiete haben deutlich höhere Hauspreise.\n",
    "\n",
    "| Metrik | Train | Test |\n",
    "|--------|-------|------|\n",
    "| R²     | 0.6358 | 0.6258 |\n",
    "| MAE    | 0.4325 | 0.4389 |\n",
    "| RMSE   | 0.5796 | 0.5831 |\n",
    "\n",
    "Der geringe Unterschied zwischen Train- und Test-R² (0.010) zeigt keine Overfitting-Tendenz. Allerdings erklärt das Modell nur ~63 % der Varianz — ein Hinweis, dass lineare Zusammenhänge ohne Skalierung und ohne nicht-lineare Terme unzureichend sind.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 3.3 Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cv = RidgeCV(alphas=np.logspace(-3, 3, 100), cv=5)\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Optimales alpha: {ridge_cv.alpha_:.6f}\\n\")\n",
    "\n",
    "result_ridge = evaluate_model(ridge_cv, X_train, X_test, y_train, y_test, \"RidgeCV (ohne Skalierung)\")\n",
    "add_result(result_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Auswertung: RidgeCV ohne Skalierung\n",
    "\n",
    "Ridge wählt ein deutlich größeres α = 0.464159, da die L2-Strafe alle Koeffizienten gleichmäßig schrumpft, statt sie auf null zu setzen. Alle 8 Features bleiben im Modell erhalten.\n",
    "\n",
    "| Metrik | Train | Test |\n",
    "|--------|-------|------|\n",
    "| R²     | 0.6465 | 0.6326 |\n",
    "| MAE    | 0.4252 | 0.4341 |\n",
    "| RMSE   | 0.5710 | 0.5778 |\n",
    "\n",
    "**Vergleich LASSO vs. Ridge (ohne Skalierung):**\n",
    "- Ridge erzielt einen leicht höheren Test-R² (+0.007) und niedrigeren RMSE (–0.0053) gegenüber dem unskalerten LASSO.\n",
    "- Der Grund liegt darin, dass LASSO `AveBedrms` komplett herauswirft, obwohl dieser Feature nach Skalierung (s. Abschnitt 3.5) eine gewisse Erklärungskraft besitzt. Ohne Skalierung dominieren Features mit großen Zahlenwerten das L1-Kriterium.\n",
    "- **Fazit:** Ohne Skalierung ist Ridge dem LASSO leicht überlegen, weil es nicht durch unterschiedliche Feature-Skalen benachteiligt wird.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 3.4 Mit MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mm, X_test_mm, y_train_mm, y_test_mm, scaler_mm, fn = get_train_test_split(df, scaler='minmax')\n",
    "\n",
    "lasso_cv_mm = LassoCV(cv=5, random_state=42, max_iter=10000)\n",
    "lasso_cv_mm.fit(X_train_mm, y_train_mm)\n",
    "\n",
    "print(f\"Optimales alpha (MinMax): {lasso_cv_mm.alpha_:.6f}\")\n",
    "print(\"\\nKoeffizienten:\")\n",
    "for name, coef in zip(fn, lasso_cv_mm.coef_):\n",
    "    print(f\"  {name:15s}: {coef:.6f}\")\n",
    "\n",
    "result_lasso_mm = evaluate_model(lasso_cv_mm, X_train_mm, X_test_mm, y_train_mm, y_test_mm, \"LassoCV (MinMax)\")\n",
    "add_result(result_lasso_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Auswertung: LassoCV mit MinMaxScaler\n",
    "\n",
    "Nach MinMax-Skalierung (alle Features auf [0, 1]) sinkt das optimale α drastisch auf 0.000078 — der Regularisierungsterm ist jetzt weit schwächer nötig, da alle Features vergleichbare Größenordnungen haben. Folglich werden **alle 8 Features beibehalten** (kein Koeffizient = 0).\n",
    "\n",
    "Die Koeffizienten sind nun direkt vergleichbar: `Longitude` (–3.679) und `Latitude` (–3.628) haben den stärksten Einfluss, gefolgt von `MedInc` (+5.542). `AveOccup` (–1.403) und `AveBedrms` (+1.255) sind ebenfalls relevant.\n",
    "\n",
    "| Metrik | Train | Test |\n",
    "|--------|-------|------|\n",
    "| R²     | 0.6465 | 0.6327 |\n",
    "| MAE    | 0.4252 | 0.4341 |\n",
    "| RMSE   | 0.5710 | 0.5778 |\n",
    "\n",
    "**Fazit:** Die Skalierung mit MinMax gleicht die Performance von LASSO an Ridge an — beide liefern nahezu identische Metriken. Das zeigt, dass LASSO ohne Skalierung systematisch schlechter abschneidet, weil es Features aufgrund ihrer Skalenunterschiede und nicht aufgrund ihrer Relevanz eliminiert.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 3.5 Mit StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ss, X_test_ss, y_train_ss, y_test_ss, scaler_ss, fn = get_train_test_split(df, scaler='standard')\n",
    "\n",
    "lasso_cv_ss = LassoCV(cv=5, random_state=42, max_iter=10000)\n",
    "lasso_cv_ss.fit(X_train_ss, y_train_ss)\n",
    "\n",
    "print(f\"Optimales alpha (Standard): {lasso_cv_ss.alpha_:.6f}\")\n",
    "print(\"\\nKoeffizienten:\")\n",
    "for name, coef in zip(fn, lasso_cv_ss.coef_):\n",
    "    print(f\"  {name:15s}: {coef:.6f}\")\n",
    "\n",
    "result_lasso_ss = evaluate_model(lasso_cv_ss, X_train_ss, X_test_ss, y_train_ss, y_test_ss, \"LassoCV (Standard)\")\n",
    "add_result(result_lasso_ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Auswertung: LassoCV mit StandardScaler\n",
    "\n",
    "Mit StandardScaler (Mittelwert 0, Standardabweichung 1) ergibt sich α = 0.000633 — ebenfalls sehr klein, da auch hier alle Features vergleichbar skaliert sind. Alle **8 Features bleiben erhalten**.\n",
    "\n",
    "Die normierten Koeffizienten zeigen: `MedInc` (+0.684) und die geographischen Features `Latitude` (–0.820) und `Longitude` (–0.739) haben den deutlichsten Einfluss. `AveOccup` (–0.225) wirkt als Belegungsdichte ebenfalls relevant.\n",
    "\n",
    "| Metrik | Train | Test |\n",
    "|--------|-------|------|\n",
    "| R²     | 0.6465 | 0.6327 |\n",
    "| MAE    | 0.4251 | 0.4340 |\n",
    "| RMSE   | 0.5710 | 0.5777 |\n",
    "\n",
    "**Vergleich MinMax vs. Standard:** Beide Skalierungen liefern quasi identische Metriken (Unterschied im Test-RMSE: 0.0001). Für LASSO ist die Wahl des Scalers bei linearen Modellen ohne Polynomtrans­formation nahezu irrelevant. StandardScaler ist jedoch für nachfolgende polynomiale Transformationen besser geeignet, da Ausreißer weniger Einfluss auf die Skalierung haben.\n",
    "\n",
    "**Fazit:** Skalierte LASSO-Modelle erreichen dieselbe Performance wie Ridge. Das deutet darauf hin, dass das lineare Modell mit diesen 8 Features ausgereizt ist und für weitere Verbesserungen nicht-lineare Erweiterungen nötig sind.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 3.6 Polynomiale Feature-Transformation + LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomiale Features (Grad 2) auf skalierten Daten\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_ss)\n",
    "X_test_poly = poly.transform(X_test_ss)\n",
    "\n",
    "print(f\"Originale Features: {X_train_ss.shape[1]}\")\n",
    "print(f\"Polynomiale Features: {X_train_poly.shape[1]}\")\n",
    "\n",
    "lasso_poly = LassoCV(cv=5, random_state=42, max_iter=10000)\n",
    "lasso_poly.fit(X_train_poly, y_train_ss)\n",
    "\n",
    "result_lasso_poly = evaluate_model(\n",
    "    lasso_poly, X_train_poly, X_test_poly, y_train_ss, y_test_ss,\n",
    "    \"LassoCV (Poly Grad 2 + Standard)\"\n",
    ")\n",
    "add_result(result_lasso_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vergleichsplot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "plot_predicted_vs_actual(\n",
    "    y_test_ss, lasso_cv_ss.predict(X_test_ss),\n",
    "    title=\"LassoCV (Standard)\", ax=axes[0]\n",
    ")\n",
    "plot_predicted_vs_actual(\n",
    "    y_test_ss, lasso_poly.predict(X_test_poly),\n",
    "    title=\"LassoCV (Poly+Standard)\", ax=axes[1]\n",
    ")\n",
    "\n",
    "fig.suptitle(\"Vergleich: Linear vs. Polynomial\", fontsize=14)\n",
    "fig.tight_layout()\n",
    "save_fig(fig, \"lasso_comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Auswertung: LassoCV mit polynomialen Features (Grad 2)\n",
    "\n",
    "Durch die Erweiterung von 8 auf **44 Features** (alle Quadrat- und Kreuzterme) kann LASSO nicht-lineare Beziehungen und Interaktionen zwischen Features erfassen. Das optimale α wird erneut durch 5-fache Kreuzvalidierung bestimmt.\n",
    "\n",
    "| Metrik | Train | Test |\n",
    "|--------|-------|------|\n",
    "| R²     | 0.7028 | 0.6993 |\n",
    "| MAE    | 0.3770 | 0.3780 |\n",
    "| RMSE   | 0.5236 | 0.5228 |\n",
    "\n",
    "**Vergleich mit LassoCV (Standard) linear:**\n",
    "- R² (Test): +0.067 (+10,6 % relative Verbesserung)\n",
    "- MAE (Test): –0.056 (–12,8 %)\n",
    "- RMSE (Test): –0.055 (–9,5 %)\n",
    "\n",
    "Das Streudiagramm zeigt deutlich engere Punktewolken um die Ideallinie, besonders im mittleren Preissegment. Im oberen Preissegment (>3) bestehen weiterhin Unterschätzungstendenzen — ein Hinweis, dass die Preisverteilung nach oben durch den Datensatz möglicherweise gekappt ist.\n",
    "\n",
    "Der Train-Test-Unterschied bleibt minimal (R² Train–Test: 0.004), obwohl die Feature-Zahl von 8 auf 44 gestiegen ist. LASSO setzt dabei viele der 44 Features auf 0 und arbeitet effektiv als Feature-Selektion.\n",
    "\n",
    "**Fazit:** Polynomiale Erweiterung Grad 2 + LASSO bringt die stärkste Verbesserung aller getesteten Konfigurationen. Die Interaktionsterme (z. B. MedInc × Latitude) erfassen geografisch-einkommensbedingte Preisunterschiede, die lineare Modelle nicht abbilden können.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 3.7 Zusammenfassung & Gesamtschlussfolgerung\n",
    "\n",
    "### Modellvergleich — Metriken (California Housing)\n",
    "\n",
    "| Modell | α | Features | R² Train | R² Test | MAE Test | RMSE Test |\n",
    "|--------|---|----------|----------|---------|----------|-----------|\n",
    "| LassoCV (ohne Skalierung) | 0.010325 | 7/8 | 0.6358 | 0.6258 | 0.4389 | 0.5831 |\n",
    "| RidgeCV (ohne Skalierung) | 0.464159 | 8/8 | 0.6465 | 0.6326 | 0.4341 | 0.5778 |\n",
    "| LassoCV (MinMax) | 0.000078 | 8/8 | 0.6465 | 0.6327 | 0.4341 | 0.5778 |\n",
    "| LassoCV (Standard) | 0.000633 | 8/8 | 0.6465 | 0.6327 | 0.4340 | 0.5777 |\n",
    "| **LassoCV (Poly Grad 2 + Standard)** | auto | **44→k** | **0.7028** | **0.6993** | **0.3780** | **0.5228** |\n",
    "\n",
    "### Erkenntnisse im Überblick\n",
    "\n",
    "| Aspekt | Erkenntnis |\n",
    "|--------|-----------|\n",
    "| Feature-Selektion via LASSO | Ohne Skalierung eliminiert LASSO `AveBedrms` fälschlicherweise aufgrund von Skalenunterschieden |\n",
    "| Skalierung | Notwendig für LASSO, damit Feature-Wichtigkeit und nicht Feature-Skala bestraft wird |\n",
    "| Ridge vs. LASSO | Ridge behält ohne Skalierung alle Features und schlägt LASSO leicht; nach Skalierung sind beide gleichwertig |\n",
    "| Polynomiale Features (Grad 2) | Stärkste Verbesserung: R²(Test) +6,7 pp, RMSE –0.055 durch Interaktionsterme |\n",
    "| Overfitting | In allen Modellen minimal (Train-Test-Lücke ≤ 0.004 R²) — LASSO regularisiert effektiv |\n",
    "\n",
    "---\n",
    "\n",
    "### Gesamtschlussfolgerung\n",
    "\n",
    "Die Untersuchung zeigt, dass **Skalierung eine Voraussetzung** für den sinnvollen Einsatz von LASSO auf dem California-Housing-Datensatz ist: Ohne Skalierung werden Features durch Skalenunterschiede eliminiert, nicht durch mangelnde Relevanz. Nach korrekter Skalierung liefern LASSO (MinMax/Standard) und Ridge nahezu identische Ergebnisse (~R² = 0.633), was darauf hindeutet, dass das lineare Modell mit den gegebenen 8 Features an seine Grenzen stößt.\n",
    "\n",
    "Der entscheidende Qualitätssprung kommt durch **polynomiale Feature-Transformation (Grad 2)**: Das Erweitern des Featureraums auf 44 Terme erlaubt es LASSO, Interaktionen wie z. B. *MedInc × Latitude* zu nutzen, die nicht-lineare geographisch-einkommensbedingte Preiszusammenhänge abbilden. Das beste Modell (LassoCV Poly Grad 2 + Standard) erreicht R²(Test) = **0.699** und RMSE = **0.523** bei vernachlässigbarem Overfitting.\n",
    "\n",
    "Dennoch verbleiben ~30 % der Varianz unerklären. Für weitere Verbesserungen bieten sich an:\n",
    "- **Polynomiale Features höheren Grades** (Grad 3, mit Vorsicht vor Overfitting)\n",
    "- **Nicht-parametrische Methoden** (z. B. kNN, Entscheidungsbäume, Ensemble-Modelle)\n",
    "- **Neuronale Netze** (Ziel des Gesamtprojekts), die beliebig komplexe Nicht-Linearitäten approximieren können\n",
    "\n",
    "Im Kontext des Gesamtprojekts (Notebook 05_Neural_Network) bilden die hier dokumentierten LASSO/Ridge-Metriken (**RMSE ≈ 0.58 linear, 0.52 polynomial**) die Baseline, an der sich neuronale Netze messen lassen müssen.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
