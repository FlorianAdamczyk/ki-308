{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 5. Ensemble-Methoden\n",
    "\n",
    "**KI1-Projekt 308** — California Housing Datensatz\n",
    "\n",
    "**Schwerpunkt P3:** Random Forest, Gradient Boosting,\n",
    "Hyperparameter-Tuning via GridSearchCV.\n",
    "\n",
    "Vorlage: Kapitel 7 Folien, Ensemble-Code aus ki_i_vl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from utils.data import load_and_clean_data, get_train_test_split\n",
    "from utils.evaluation import evaluate_model, add_result\n",
    "from utils.plotting import (\n",
    "    plot_predicted_vs_actual, plot_residuals,\n",
    "    plot_feature_importances, save_fig,\n",
    ")\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 5.1 Daten laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_and_clean_data()\n",
    "X_train, X_test, y_train, y_test, feature_names = get_train_test_split(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 5.2 Random Forest — Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "result_rf = evaluate_model(rf, X_train, X_test, y_train, y_test, \"Random Forest (100 Trees)\")\n",
    "add_result(result_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 5.3 Random Forest — Hyperparameter-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "\n",
    "# Hinweis: GridSearchCV dauert je nach Rechner einige Minuten\n",
    "grid_rf = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "    param_grid_rf,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Beste Parameter: {grid_rf.best_params_}\")\n",
    "print(f\"Bester CV R²: {grid_rf.best_score_:.4f}\")\n",
    "\n",
    "result_rf_tuned = evaluate_model(\n",
    "    grid_rf.best_estimator_, X_train, X_test, y_train, y_test,\n",
    "    \"Random Forest (tuned)\"\n",
    ")\n",
    "add_result(result_rf_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_feature_importances(\n",
    "    grid_rf.best_estimator_.feature_importances_, feature_names,\n",
    "    title=\"Random Forest: Feature Importances\",\n",
    "    save_name=\"rf_feature_importances\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 5.4 Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    ")\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "result_gb = evaluate_model(gb, X_train, X_test, y_train, y_test, \"Gradient Boosting (200 Trees)\")\n",
    "add_result(result_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 5.5 Gradient Boosting — Hyperparameter-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_gb = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "}\n",
    "\n",
    "grid_gb = GridSearchCV(\n",
    "    GradientBoostingRegressor(random_state=42),\n",
    "    param_grid_gb,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "grid_gb.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Beste Parameter: {grid_gb.best_params_}\")\n",
    "print(f\"Bester CV R²: {grid_gb.best_score_:.4f}\")\n",
    "\n",
    "result_gb_tuned = evaluate_model(\n",
    "    grid_gb.best_estimator_, X_train, X_test, y_train, y_test,\n",
    "    \"Gradient Boosting (tuned)\"\n",
    ")\n",
    "add_result(result_gb_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vergleichsplot: Random Forest vs. Gradient Boosting\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "plot_predicted_vs_actual(\n",
    "    y_test, grid_rf.best_estimator_.predict(X_test),\n",
    "    title=\"Random Forest (tuned)\", ax=axes[0]\n",
    ")\n",
    "plot_predicted_vs_actual(\n",
    "    y_test, grid_gb.best_estimator_.predict(X_test),\n",
    "    title=\"Gradient Boosting (tuned)\", ax=axes[1]\n",
    ")\n",
    "\n",
    "fig.suptitle(\"Ensemble-Methoden: Predicted vs. Actual\", fontsize=14)\n",
    "fig.tight_layout()\n",
    "save_fig(fig, \"ensemble_comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 5.6 Zusammenfassung\n",
    "\n",
    "| Erkenntnis | Detail |\n",
    "|-----------|--------|\n",
    "| Random Forest | Deutlich besser als einzelner Decision Tree |\n",
    "| Gradient Boosting | Oft bestes Ensemble-Verfahren |\n",
    "| Feature Importance | Konsistent: MedInc dominiert |\n",
    "| Hyperparameter | Tuning lohnt sich, besonders learning_rate bei GB |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
